## ğŸ¤– Assistant

å¥½çš„ï¼Œéµç…§æ‚¨çš„è¦æ±‚ï¼Œæˆ‘å°†ä¸ºæ‚¨ç¼–å†™é€‚é…å™¨å±‚çº§æ–¹æ¡ˆçš„æ¡†æ¶ä¸ç¤ºèŒƒä»£ç ã€‚

è¿™ä¸ªå®ç°å°†åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š

1. **æŠ½è±¡åŸºç±» (`BaseLLMService`)**ï¼šå®šä¹‰æ‰€æœ‰æœåŠ¡å¿…é¡»éµå®ˆçš„ç»Ÿä¸€æ¥å£å’Œå…±äº«åŠŸèƒ½ã€‚
2. **å…·ä½“æœåŠ¡å®ç° (`OpenAIService`, `GeminiService`)**ï¼šä¸¤ä¸ªç‹¬ç«‹çš„æœåŠ¡å•†å®ç°ï¼Œå±•ç¤ºå¦‚ä½•æ¥å…¥ä¸åŒç±»å‹çš„APIã€‚
3. **é€‚é…å™¨ (`DashScopeAdapter`)**ï¼šå±•ç¤ºå¦‚ä½•é€šè¿‡é€‚é…å™¨æ¨¡å¼ï¼Œå¤ç”¨ç°æœ‰æœåŠ¡å®ç°æ¥å¿«é€Ÿæ”¯æŒä¸€ä¸ªAPIå…¼å®¹çš„æ–°æœåŠ¡å•†ã€‚
4. **å·¥å‚ (`LLMFactory`)**ï¼šè´Ÿè´£æ ¹æ®é…ç½®åŠ¨æ€åˆ›å»ºå’Œåˆ†å‘æ­£ç¡®çš„æœåŠ¡å®ä¾‹ã€‚
5. **ä½¿ç”¨ç¤ºä¾‹ (`main.py`)**ï¼šä¸€ä¸ªç®€å•çš„å‘½ä»¤è¡Œè„šæœ¬ï¼Œæ¼”ç¤ºå¦‚ä½•é€šè¿‡å·¥å‚è°ƒç”¨ä¸åŒçš„æœåŠ¡ï¼Œå¹¶éªŒè¯å…¶è¾“å‡ºã€‚

---

### 1. é¡¹ç›®æ–‡ä»¶ç»“æ„

```
src/
â”œâ”€â”€ llm_services/
â”‚   â”œâ”€â”€ __init__.py               # åŒ…åˆå§‹åŒ–æ–‡ä»¶
â”‚   â”œâ”€â”€ base_service.py           # æŠ½è±¡åŸºç±»
â”‚   â”œâ”€â”€ openai_service.py         # OpenAI æœåŠ¡å®ç°
â”‚   â”œâ”€â”€ gemini_service.py         # Gemini æœåŠ¡å®ç°
â”‚   â””â”€â”€ dashscope_adapter.py      # DashScope é€‚é…å™¨
â”‚
â”œâ”€â”€ llm_factory.py                # LLM æœåŠ¡å·¥å‚
â””â”€â”€ main.py                       # ç¤ºä¾‹ï¼šå¦‚ä½•ä½¿ç”¨å·¥å‚
```

---

### 2. æ¡†æ¶ä»£ç å®ç°

#### `src/llm_services/base_service.py`
è¿™æ˜¯æˆ‘ä»¬è®¾è®¡çš„æ ¸å¿ƒï¼Œå®šä¹‰äº†æ‰€æœ‰LLMæœåŠ¡å¿…é¡»éµå®ˆçš„å¥‘çº¦ã€‚

<!-- [START OF FILE: src/llm_services/base_service.py] -->
```python
# src/llm_services/base_service.py

import logging
from abc import ABC, abstractmethod
from typing import Dict, Any, Tuple, List

class BaseLLMService(ABC):
    """
    LLMæœåŠ¡æŠ½è±¡åŸºç±» (The Contract)

    å®šä¹‰äº†æ‰€æœ‰LLMæœåŠ¡å®ç°å¿…é¡»éµå®ˆçš„ç»Ÿä¸€æ¥å£ï¼Œå¹¶æä¾›äº†
    æ‰€æœ‰æœåŠ¡å…±äº«çš„åŸºç¡€åŠŸèƒ½ï¼ˆå¦‚é…ç½®å¤„ç†ã€æ—¥å¿—è®°å½•ç­‰ï¼‰ã€‚
    """

    def __init__(self, config: Dict[str, Any], model_config_name: str):
        """
        åˆå§‹åŒ–åŸºç±»ã€‚
        
        Args:
            config: ä»é…ç½®æ–‡ä»¶åŠ è½½çš„è¯¥æ¨¡å‹çš„å®Œæ•´é…ç½®å­—å…¸ã€‚
            model_config_name: æ¨¡å‹é…ç½®çš„åˆ«å (ä¾‹å¦‚ 'gemini-pro-1.5')ã€‚
        """
        self.config = config
        self.model_config_name = model_config_name
        self.logger = logging.getLogger(self.__class__.__name__)

        # --- æå–é€šç”¨é…ç½® ---
        self.provider = self.config.get('provider', 'unknown')
        self.model = self.config.get('model_name')
        self.api_key = self.config.get('api_key')
        self.base_url = self.config.get('base_url')

        if not self.model or not self.api_key:
            raise ValueError(f"æ¨¡å‹é…ç½® '{model_config_name}' å¿…é¡»åŒ…å« 'model_name' å’Œ 'api_key'ã€‚")

        self.logger.info(f"åŸºç¡€æœåŠ¡ '{self.model_config_name}' åˆå§‹åŒ–...")

    @abstractmethod
    async def annotate_poem(self, poem_content: str, emotion_schema: str) -> Dict[str, Any]:
        """
        [æŠ½è±¡æ–¹æ³•] å¯¹è¯—æ­Œå†…å®¹è¿›è¡Œæƒ…æ„Ÿæ ‡æ³¨ã€‚

        æ‰€æœ‰å­ç±»å¿…é¡»å®ç°æ­¤æ–¹æ³•ï¼Œä»¥å®šä¹‰å¦‚ä½•è°ƒç”¨å…¶ç‰¹å®šçš„LLM APIã€‚

        Args:
            poem_content: å¾…æ ‡æ³¨çš„è¯—æ­ŒåŸæ–‡ã€‚
            emotion_schema: æƒ…æ„Ÿåˆ†ç±»ä½“ç³»çš„æè¿°æ–‡æœ¬ã€‚

        Returns:
            ä¸€ä¸ªåŒ…å«æ ‡æ³¨ç»“æœçš„å­—å…¸ã€‚
        """
        pass

    @abstractmethod
    async def health_check(self) -> Tuple[bool, str]:
        """
        [æŠ½è±¡æ–¹æ³•] æ‰§è¡Œå¯¹LLMæœåŠ¡çš„å¥åº·æ£€æŸ¥ã€‚

        ç”¨äºéªŒè¯APIå¯†é’¥ã€ç½‘ç»œè¿æ¥å’ŒåŸºç¡€é…ç½®æ˜¯å¦æœ‰æ•ˆã€‚

        Returns:
            ä¸€ä¸ªå…ƒç»„ (is_healthy: bool, message: str)ã€‚
        """
        pass

    def get_service_info(self) -> Dict[str, Any]:
        """
        è·å–å½“å‰æœåŠ¡çš„åŸºç¡€é…ç½®ä¿¡æ¯ï¼ˆæ•æ„Ÿä¿¡æ¯å·²è„±æ•ï¼‰ã€‚
        å­ç±»å¯ä»¥é‡å†™æ­¤æ–¹æ³•ä»¥æ·»åŠ æ›´å¤šç‰¹å®šä¿¡æ¯ã€‚
        """
        return self._mask_sensitive_data({
            "provider": self.provider,
            "model_config_name": self.model_config_name,
            "model_name": self.model,
            "base_url": self.base_url,
            "api_key": self.api_key
        })

    def _mask_api_key(self, api_key: str) -> str:
        """å¯¹APIå¯†é’¥è¿›è¡Œè„±æ•å¤„ç†ï¼Œç”¨äºæ—¥å¿—è¾“å‡ºã€‚"""
        if not api_key or not isinstance(api_key, str) or len(api_key) <= 8:
            return "****"
        return f"{api_key[:4]}...{api_key[-4:]}"

    def _mask_sensitive_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """é€’å½’åœ°å¯¹å­—å…¸ä¸­çš„'api_key'å­—æ®µè¿›è¡Œè„±æ•ã€‚"""
        masked_data = data.copy()
        if 'api_key' in masked_data:
            masked_data['api_key'] = self._mask_api_key(masked_data['api_key'])
        return masked_data

    # --- å¯ä»¥åœ¨æ­¤æ·»åŠ æ›´å¤šå…±äº«æ–¹æ³•ï¼Œå¦‚ç»Ÿä¸€çš„æç¤ºè¯æ„å»ºã€å“åº”éªŒè¯ç­‰ ---
    def _build_prompt(self, poem_content: str, emotion_schema: str) -> str:
        """ä¸€ä¸ªç®€å•çš„å…±äº«æç¤ºè¯æ„å»ºé€»è¾‘ç¤ºä¾‹ã€‚"""
        return f"""
ä½ æ˜¯ä¸€ä¸ªæƒ…æ„Ÿåˆ†æä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹æƒ…æ„Ÿåˆ†ç±»ä½“ç³»ï¼Œä¸ºä¸‹é¢çš„è¯—æ­Œå†…å®¹è¿›è¡Œæ ‡æ³¨ã€‚
æƒ…æ„Ÿä½“ç³»:
{emotion_schema}

è¯—æ­Œå†…å®¹:
{poem_content}

è¯·ä»¥JSONæ ¼å¼è¿”å›ä½ çš„åˆ†æç»“æœã€‚
"""

```
<!-- [END OF FILE: src/llm_services/base_service.py] -->

#### `src/llm_services/openai_service.py`
ç¬¬ä¸€ä¸ªå…·ä½“å®ç°ï¼Œç”¨äºæ‰€æœ‰å…¼å®¹OpenAI APIçš„æœåŠ¡ã€‚æ³¨æ„ `annotate_poem` æ–¹æ³•ä¸­çš„ `extra_params`ï¼Œè¿™æ˜¯é€‚é…å™¨æ¨¡å¼èƒ½å¤Ÿå·¥ä½œçš„å…³é”®ã€‚

<!-- [START OF FILE: src/llm_services/openai_service.py] -->
```python
# src/llm_services/openai_service.py

from typing import Dict, Any, Optional, Tuple
from openai import AsyncOpenAI, OpenAIError
from .base_service import BaseLLMService

class OpenAIService(BaseLLMService):
    """
    OpenAI API æœåŠ¡å®ç°ã€‚
    
    æ­¤ç±»å¯ç”¨äºä¸å®˜æ–¹OpenAI APIæˆ–ä»»ä½•å…¶ä»–å…¼å®¹OpenAI APIçš„å¹³å°ï¼ˆå¦‚Azure, SiliconFlowï¼‰è¿›è¡Œäº¤äº’ã€‚
    """
    def __init__(self, config: Dict[str, Any], model_config_name: str):
        super().__init__(config, model_config_name)
        
        # --- è§£æOpenAIç‰¹å®šé…ç½® ---
        self.temperature = float(self.config.get('temperature', 0.5))
        self.timeout = int(self.config.get('timeout', 60))
        
        # --- åˆå§‹åŒ–å®¢æˆ·ç«¯ ---
        try:
            self.client = AsyncOpenAI(
                api_key=self.api_key,
                base_url=self.base_url,
                timeout=self.timeout,
            )
            self.logger.info(f"OpenAIå…¼å®¹å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆã€‚URL: {self.base_url}, æ¨¡å‹: {self.model}")
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯å¤±è´¥: {e}", exc_info=True)
            raise

    async def annotate_poem(self, poem_content: str, emotion_schema: str, extra_params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        ä½¿ç”¨OpenAIçš„Chat Completion APIè¿›è¡Œæ ‡æ³¨ã€‚
        
        Args:
            poem_content: è¯—æ­ŒåŸæ–‡ã€‚
            emotion_schema: æƒ…æ„Ÿä½“ç³»æè¿°ã€‚
            extra_params (Optional): ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«éœ€è¦åˆå¹¶åˆ°APIè¯·æ±‚ä½“é¡¶å±‚çš„é¢å¤–å‚æ•°ã€‚
                                     è¿™æ˜¯ä¸ºé€‚é…å™¨æ¨¡å¼è®¾è®¡çš„å…³é”®æ‰©å±•ç‚¹ã€‚
        """
        try:
            full_prompt = self._build_prompt(poem_content, emotion_schema)
            
            request_data = {
                "model": self.model,
                "messages": [{"role": "user", "content": full_prompt}],
                "temperature": self.temperature,
                "response_format": {"type": "json_object"}
            }

            # å¦‚æœé€‚é…å™¨ä¼ å…¥äº†é¢å¤–å‚æ•°ï¼Œåˆ™åˆå¹¶å®ƒä»¬
            if extra_params:
                self.logger.debug(f"åˆå¹¶æ¥è‡ªé€‚é…å™¨çš„é¢å¤–å‚æ•°: {extra_params}")
                request_data.update(extra_params)
            
            self.logger.info(f"å‘ {self.provider} APIå‘é€è¯·æ±‚...")
            self.logger.debug(f"è¯·æ±‚æ•°æ®: {request_data}")

            response = await self.client.chat.completions.create(**request_data)
            
            response_text = response.choices[0].message.content
            
            self.logger.info(f"æˆåŠŸä» {self.provider} æ¥æ”¶åˆ°å“åº”ã€‚")
            
            # åœ¨å®é™…é¡¹ç›®ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨ä¸€ä¸ªJSONè§£æå’ŒéªŒè¯æ¨¡å—
            import json
            return json.loads(response_text)

        except OpenAIError as e:
            self.logger.error(f"{self.provider} APIè°ƒç”¨å¤±è´¥: {e}", exc_info=True)
            raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸Šå±‚å¤„ç†
        except Exception as e:
            self.logger.error(f"å¤„ç† {self.provider} å“åº”æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            raise

    async def health_check(self) -> Tuple[bool, str]:
        """ä½¿ç”¨åˆ—å‡ºæ¨¡å‹APIçš„è½»é‡çº§è°ƒç”¨æ¥æ£€æŸ¥å¥åº·çŠ¶å†µã€‚"""
        try:
            await self.client.models.list(timeout=10)
            msg = f"æˆåŠŸè¿æ¥åˆ° {self.provider} å¹¶éªŒè¯APIå¯†é’¥ã€‚"
            self.logger.info(msg)
            return True, msg
        except Exception as e:
            msg = f"è¿æ¥ {self.provider} å¤±è´¥: {e}"
            self.logger.error(msg)
            return False, msg

    def get_service_info(self) -> Dict[str, Any]:
        info = super().get_service_info()
        info.update({
            "temperature": self.temperature,
            "timeout": self.timeout
        })
        return self._mask_sensitive_data(info)

```
<!-- [END OF FILE: src/llm_services/openai_service.py] -->

#### `src/llm_services/gemini_service.py`
ç¬¬äºŒä¸ªå…·ä½“å®ç°ï¼Œå±•ç¤ºäº†å¦‚ä½•æ¥å…¥ä¸€ä¸ªAPIå’ŒSDKå®Œå…¨ä¸åŒçš„æœåŠ¡ã€‚

<!-- [START OF FILE: src/llm_services/gemini_service.py] -->
```python
# src/llm_services/gemini_service.py

from typing import Dict, Any, Tuple
import google.generativeai as genai
from .base_service import BaseLLMService

class GeminiService(BaseLLMService):
    """Google Gemini API æœåŠ¡å®ç°ã€‚"""
    
    def __init__(self, config: Dict[str, Any], model_config_name: str):
        super().__init__(config, model_config_name)
        
        # --- è§£æGeminiç‰¹å®šé…ç½® ---
        self.temperature = float(self.config.get('temperature', 0.5))
        
        # --- åˆå§‹åŒ–å®¢æˆ·ç«¯ ---
        try:
            genai.configure(api_key=self.api_key)
            self.genai_model = genai.GenerativeModel(
                self.model,
                generation_config=genai.types.GenerationConfig(
                    temperature=self.temperature,
                    response_mime_type="application/json" # è¯·æ±‚JSONè¾“å‡º
                )
            )
            self.logger.info(f"Geminiå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆã€‚æ¨¡å‹: {self.model}")
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–Geminiå®¢æˆ·ç«¯å¤±è´¥: {e}", exc_info=True)
            raise
            
    async def annotate_poem(self, poem_content: str, emotion_schema: str) -> Dict[str, Any]:
        """ä½¿ç”¨Geminiçš„generate_content_asyncè¿›è¡Œæ ‡æ³¨ã€‚"""
        try:
            full_prompt = self._build_prompt(poem_content, emotion_schema)
            
            self.logger.info(f"å‘ {self.provider} APIå‘é€è¯·æ±‚...")
            self.logger.debug(f"è¯·æ±‚Prompté•¿åº¦: {len(full_prompt)}")
            
            response = await self.genai_model.generate_content_async(full_prompt)
            
            # Gemini SDKåœ¨é…ç½®äº†JSONè¾“å‡ºæ—¶ï¼Œä¼šè‡ªåŠ¨è§£æ`response.text`ä¸ºJSON
            # ä½†æˆ‘ä»¬éœ€è¦ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            response_text = response.text.strip().replace("```json", "").replace("```", "").strip()

            self.logger.info(f"æˆåŠŸä» {self.provider} æ¥æ”¶åˆ°å“åº”ã€‚")
            
            import json
            return json.loads(response_text)

        except Exception as e:
            self.logger.error(f"å¤„ç† {self.provider} å“åº”æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            raise
    
    async def health_check(self) -> Tuple[bool, str]:
        """ä½¿ç”¨åˆ—å‡ºæ¨¡å‹APIæ¥æ£€æŸ¥å¥åº·çŠ¶å†µã€‚"""
        try:
            # è½»é‡çº§çš„è°ƒç”¨
            models = genai.list_models()
            if any(self.model in m.name for m in models):
                msg = f"æˆåŠŸè¿æ¥åˆ° {self.provider}ï¼ŒAPIå¯†é’¥æœ‰æ•ˆä¸”æ¨¡å‹ '{self.model}' å¯ç”¨ã€‚"
                self.logger.info(msg)
                return True, msg
            else:
                 msg = f"è¿æ¥åˆ° {self.provider} æˆåŠŸï¼Œä½†æ¨¡å‹ '{self.model}' ä¸å¯ç”¨ã€‚"
                 self.logger.warning(msg)
                 return False, msg
        except Exception as e:
            msg = f"è¿æ¥ {self.provider} å¤±è´¥: {e}"
            self.logger.error(msg)
            return False, msg

    def get_service_info(self) -> Dict[str, Any]:
        info = super().get_service_info()
        info.update({"temperature": self.temperature})
        return self._mask_sensitive_data(info)

```
<!-- [END OF FILE: src/llm_services/gemini_service.py] -->

#### `src/llm_services/dashscope_adapter.py`
é€‚é…å™¨å±‚ã€‚å®ƒç»§æ‰¿ `BaseLLMService` ä»¥å¯¹å¤–ç•Œä¿æŒç»Ÿä¸€æ¥å£ï¼Œä½†å†…éƒ¨ç»„åˆå¹¶å§”æ‰˜ `OpenAIService` æ¥å®Œæˆå¤§éƒ¨åˆ†å·¥ä½œã€‚

<!-- [START OF FILE: src/llm_services/dashscope_adapter.py] -->
```python
# src/llm_services/dashscope_adapter.py

from typing import Dict, Any, Tuple
from .base_service import BaseLLMService
from .openai_service import OpenAIService

class DashScopeAdapter(BaseLLMService):
    """
    é˜¿é‡Œäº‘ç™¾ç‚¼(DashScope) APIæœåŠ¡é€‚é…å™¨ã€‚

    è¯¥ç±»æ¼”ç¤ºäº†é€‚é…å™¨æ¨¡å¼ï¼š
    1. å®ƒå®ç°äº† `BaseLLMService` æ¥å£ï¼Œå› æ­¤å¯¹å®¢æˆ·ç«¯æ¥è¯´æ˜¯é€æ˜çš„ã€‚
    2. å®ƒå†…éƒ¨åŒ…å«ä¸€ä¸ª `OpenAIService` å®ä¾‹ï¼ˆç»„åˆï¼‰ã€‚
    3. å½“å…¶æ–¹æ³•è¢«è°ƒç”¨æ—¶ï¼Œå®ƒä¼šå¤„ç†DashScopeç‰¹æœ‰çš„é€»è¾‘ï¼Œç„¶åå°†
       æ ¸å¿ƒAPIè°ƒç”¨å§”æ‰˜ç»™å†…éƒ¨çš„ `OpenAIService` å®ä¾‹ã€‚
    """

    def __init__(self, config: Dict[str, Any], model_config_name: str):
        super().__init__(config, model_config_name)
        
        # --- è§£æDashScopeç‰¹å®šé…ç½® ---
        self.enable_search = str(self.config.get('enable_search', 'false')).lower() == 'true'
        
        # --- åˆ›å»ºè¢«é€‚é…çš„å¯¹è±¡ ---
        # DashScopeçš„ API ä¸ OpenAI é«˜åº¦å…¼å®¹ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å¤ç”¨ OpenAIServiceã€‚
        # æˆ‘ä»¬å°†å½“å‰é…ç½®ä¼ é€’ç»™å®ƒï¼Œå› ä¸ºå®ƒåŒ…å«äº†æ‰€æœ‰å¿…è¦çš„å…¼å®¹å‚æ•°
        # (api_key, base_url, model_name, temperatureç­‰)ã€‚
        self._openai_service = OpenAIService(config, model_config_name)
        self.logger.info("DashScopeé€‚é…å™¨åˆå§‹åŒ–å®Œæˆï¼Œå†…éƒ¨ä½¿ç”¨OpenAIå…¼å®¹æœåŠ¡ã€‚")

    async def annotate_poem(self, poem_content: str, emotion_schema: str) -> Dict[str, Any]:
        """
        å®ç°æ ‡æ³¨æ¥å£ï¼Œå‡†å¤‡ç‰¹æœ‰å‚æ•°å¹¶å§”æ‰˜ç»™OpenAIServiceã€‚
        """
        self.logger.info("é€šè¿‡DashScopeé€‚é…å™¨æ‰§è¡Œæ ‡æ³¨...")
        
        # 1. å‡†å¤‡DashScopeç‰¹æœ‰çš„å‚æ•°
        dashscope_extra_params = {
            "result_format": "message",
        }
        if self.enable_search:
            dashscope_extra_params["enable_search"] = self.enable_search
            
        # 2. å°†æ ¸å¿ƒä»»åŠ¡å§”æ‰˜ç»™å†…éƒ¨çš„OpenAIServiceå®ä¾‹ï¼Œå¹¶é€šè¿‡`extra_params`æ³¨å…¥ç‰¹æœ‰å‚æ•°
        return await self._openai_service.annotate_poem(
            poem_content,
            emotion_schema,
            extra_params=dashscope_extra_params
        )

    async def health_check(self) -> Tuple[bool, str]:
        """ç›´æ¥å§”æ‰˜å¥åº·æ£€æŸ¥ä»»åŠ¡ã€‚"""
        self.logger.info("é€šè¿‡DashScopeé€‚é…å™¨æ‰§è¡Œå¥åº·æ£€æŸ¥...")
        # åº•å±‚APIå…¼å®¹ï¼Œæ‰€ä»¥å¥åº·æ£€æŸ¥é€»è¾‘ä¹Ÿå®Œå…¨å¯ä»¥å¤ç”¨
        return await self._openai_service.health_check()
        
    def get_service_info(self) -> Dict[str, Any]:
        # ä»å§”æ‰˜çš„æœåŠ¡è·å–åŸºç¡€ä¿¡æ¯ï¼Œå¹¶æ·»åŠ é€‚é…å™¨è‡ªå·±çš„ä¿¡æ¯
        info = self._openai_service.get_service_info()
        info['provider'] = self.provider # è¦†ç›–ä¸º 'dashscope'
        info['adapter_specific_params'] = {
            "enable_search": self.enable_search
        }
        return self._mask_sensitive_data(info)
```
<!-- [END OF FILE: src/llm_services/dashscope_adapter.py] -->

#### `src/llm_services/__init__.py`
è®© `llm_services` ç›®å½•æˆä¸ºä¸€ä¸ªå¯å¯¼å…¥çš„åŒ…ã€‚

<!-- [START OF FILE: src/llm_services/__init__.py] -->
```python
# src/llm_services/__init__.py

from .base_service import BaseLLMService
from .openai_service import OpenAIService
from .gemini_service import GeminiService
from .dashscope_adapter import DashScopeAdapter

# è¿™ä½¿å¾—æˆ‘ä»¬å¯ä»¥é€šè¿‡ from src.llm_services import OpenAIService çš„æ–¹å¼å¯¼å…¥
__all__ = [
    "BaseLLMService",
    "OpenAIService",
    "GeminiService",
    "DashScopeAdapter",
]
```
<!-- [END OF FILE: src/llm_services/__init__.py] -->

#### `src/llm_factory.py`
å·¥å‚ç±»ï¼Œæ˜¯è§£è€¦å®¢æˆ·ç«¯å’Œå…·ä½“æœåŠ¡å®ç°çš„æ ¸å¿ƒã€‚

<!-- [START OF FILE: src/llm_factory.py] -->
```python
# src/llm_factory.py

import logging
from typing import Dict, Type
from .llm_services import BaseLLMService, OpenAIService, GeminiService, DashScopeAdapter

# å‡è®¾æœ‰ä¸€ä¸ªConfigManageræ¥è¯»å–é…ç½®æ–‡ä»¶
class SimpleConfigManager:
    def __init__(self):
        # åœ¨çœŸå®é¡¹ç›®ä¸­ï¼Œè¿™é‡Œä¼šè¯»å– .ini æˆ– .yaml æ–‡ä»¶
        self._configs = {
            "gpt-4o-mini": {
                "provider": "openai",
                "model_name": "gpt-4o-mini",
                "api_key": "sk-your-openai-key",
                "base_url": "https://api.openai.com/v1/",
            },
            "gemini-1.5-flash": {
                "provider": "gemini",
                "model_name": "gemini-1.5-flash-latest",
                "api_key": "your-gemini-api-key",
            },
            "qwen-turbo": {
                "provider": "dashscope",
                "model_name": "qwen-turbo",
                "api_key": "sk-your-dashscope-key",
                "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
                "enable_search": "true",
            }
        }
    
    def get_model_config(self, config_name: str) -> Dict:
        if config_name not in self._configs:
            raise ValueError(f"æœªæ‰¾åˆ°åä¸º '{config_name}' çš„æ¨¡å‹é…ç½®ã€‚")
        return self._configs[config_name]

# å…¨å±€é…ç½®ç®¡ç†å™¨å®ä¾‹
config_manager = SimpleConfigManager()


class LLMFactory:
    """LLMæœåŠ¡å·¥å‚ï¼Œæ ¹æ®é…ç½®åˆ›å»ºå¹¶è¿”å›ç›¸åº”çš„æœåŠ¡å®ä¾‹ã€‚"""

    def __init__(self):
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # æ ¸å¿ƒæ˜ å°„ï¼šå°†providerå­—ç¬¦ä¸²æ˜ å°„åˆ°å¯¹åº”çš„æœåŠ¡ç±»
        self._providers: Dict[str, Type[BaseLLMService]] = {
            'openai': OpenAIService,
            'gemini': GeminiService,
            'dashscope': DashScopeAdapter, # æ³¨æ„è¿™é‡Œæ³¨å†Œçš„æ˜¯é€‚é…å™¨
        }
        self.logger.info(f"LLMå·¥å‚åˆå§‹åŒ–å®Œæˆï¼Œæ”¯æŒçš„æä¾›å•†: {list(self._providers.keys())}")

    def get_llm_service(self, config_name: str) -> BaseLLMService:
        """
        ä¸»æ–¹æ³•ï¼šè·å–ä¸€ä¸ªLLMæœåŠ¡å®ä¾‹ã€‚
        
        Args:
            config_name: åœ¨é…ç½®æ–‡ä»¶ä¸­å®šä¹‰çš„æ¨¡å‹åˆ«åã€‚
            
        Returns:
            ä¸€ä¸ªéµå¾ª BaseLLMService æ¥å£çš„æœåŠ¡å®ä¾‹ã€‚
        """
        self.logger.info(f"è¯·æ±‚åˆ›å»ºLLMæœåŠ¡ï¼Œé…ç½®å: '{config_name}'...")
        try:
            model_config = config_manager.get_model_config(config_name)
            provider_name = model_config.get('provider')
            
            if not provider_name:
                raise ValueError(f"é…ç½® '{config_name}' ä¸­ç¼ºå°‘ 'provider' å­—æ®µã€‚")
            
            service_class = self._providers.get(provider_name.lower())
            
            if not service_class:
                raise ValueError(f"ä¸æ”¯æŒçš„æä¾›å•†: '{provider_name}'ã€‚")

            # åŠ¨æ€åˆ›å»ºå®ä¾‹
            service_instance = service_class(model_config, config_name)
            self.logger.info(f"æˆåŠŸåˆ›å»º '{provider_name}' æœåŠ¡å®ä¾‹ï¼Œæ¨¡å‹ä¸º '{service_instance.model}'ã€‚")
            
            return service_instance
            
        except Exception as e:
            self.logger.error(f"åˆ›å»ºLLMæœåŠ¡ '{config_name}' å¤±è´¥: {e}", exc_info=True)
            raise

# å…¨å±€å·¥å‚å®ä¾‹
llm_factory = LLMFactory()

```
<!-- [END OF FILE: src/llm_factory.py] -->

---

### 3. ä½¿ç”¨ç¤ºä¾‹

#### `src/main.py`
è¿™ä¸ªæ–‡ä»¶æ¼”ç¤ºäº†ä¸Šå±‚è°ƒç”¨è€…ï¼ˆå¦‚ `Annotator`ï¼‰å¦‚ä½•ä¸è¿™ä¸ªæ¡†æ¶äº¤äº’ã€‚å®ƒæ ¹æœ¬ä¸éœ€è¦çŸ¥é“ `OpenAIService` æˆ– `DashScopeAdapter` çš„å­˜åœ¨ï¼Œåªéœ€è¦é€šè¿‡é…ç½®åç§°å‘å·¥å‚è¯·æ±‚æœåŠ¡å³å¯ã€‚

<!-- [START OF FILE: src/main.py] -->
```python
# src/main.py

import asyncio
import logging
from llm_factory import llm_factory

# --- é…ç½®æ—¥å¿— ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# --- æ¨¡æ‹Ÿçš„ä¸šåŠ¡æ•°æ® ---
POEM_CONTENT = "åºŠå‰æ˜æœˆå…‰ï¼Œç–‘æ˜¯åœ°ä¸Šéœœã€‚ä¸¾å¤´æœ›æ˜æœˆï¼Œä½å¤´æ€æ•…ä¹¡ã€‚"
EMOTION_SCHEMA = "è¯·åˆ†æä¸»è¦æƒ…æ„Ÿå’Œæ¬¡è¦æƒ…æ„Ÿã€‚ä¾‹å¦‚ï¼š{'primary': 'æ€ä¹¡', 'secondary': ['å®é™', 'å­¤ç‹¬']}"


async def test_service(config_name: str):
    """
    ä¸€ä¸ªé€šç”¨çš„æµ‹è¯•å‡½æ•°ï¼Œå®ƒæ¥æ”¶ä¸€ä¸ªé…ç½®åç§°ï¼Œ
    ç„¶åä½¿ç”¨å·¥å‚åˆ›å»ºæœåŠ¡å¹¶æ‰§è¡Œä»»åŠ¡ï¼Œå®Œå…¨ä¸çŸ¥é“åº•å±‚å®ç°ã€‚
    """
    print("\n" + "="*50)
    print(f"ğŸš€ å¼€å§‹æµ‹è¯•æœåŠ¡: {config_name}")
    print("="*50)
    
    try:
        # 1. ä»å·¥å‚è·å–æœåŠ¡å®ä¾‹ (å®¢æˆ·ç«¯åªä¸å·¥å‚å’ŒBaseLLMServiceæ¥å£äº¤äº’)
        service = llm_factory.get_llm_service(config_name)

        # 2. æ‰“å°æœåŠ¡ä¿¡æ¯
        print("ğŸ” æœåŠ¡ä¿¡æ¯:")
        print(service.get_service_info())

        # 3. æ‰§è¡Œå¥åº·æ£€æŸ¥
        print("\nğŸ©º æ‰§è¡Œå¥åº·æ£€æŸ¥...")
        is_healthy, message = await service.health_check()
        print(f"å¥åº·çŠ¶æ€: {'âœ… å¥åº·' if is_healthy else 'âŒ å¤±è´¥'}")
        print(f"æ¶ˆæ¯: {message}")

        if not is_healthy:
            print(f"æœåŠ¡ '{config_name}' æœªé€šè¿‡å¥åº·æ£€æŸ¥ï¼Œè·³è¿‡æ ‡æ³¨ã€‚")
            return

        # 4. æ‰§è¡Œæ ¸å¿ƒä¸šåŠ¡é€»è¾‘
        print("\nğŸ“ æ‰§è¡Œè¯—æ­Œæ ‡æ³¨...")
        annotation_result = await service.annotate_poem(POEM_CONTENT, EMOTION_SCHEMA)
        
        print("\nâœ¨ æ ‡æ³¨ç»“æœ:")
        print(annotation_result)
        
    except Exception as e:
        logging.error(f"æµ‹è¯•æœåŠ¡ '{config_name}' æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
    finally:
        print(f"ğŸ å®Œæˆæµ‹è¯•: {config_name}")


async def main():
    # æˆ‘ä»¬æƒ³è¦æµ‹è¯•çš„æ‰€æœ‰æ¨¡å‹é…ç½®
    # æ³¨æ„ï¼šä½ éœ€è¦å°† llm_factory.py ä¸­çš„ API Key æ›¿æ¢ä¸ºä½ çš„çœŸå®å¯†é’¥æ‰èƒ½æˆåŠŸè¿è¡Œ
    model_configs_to_test = [
        "gpt-4o-mini",
        "gemini-1.5-flash",
        "qwen-turbo"  # è¿™ä¸ªé…ç½®å°†é€šè¿‡DashScopeAdapter
    ]
    
    for config_name in model_configs_to_test:
        await test_service(config_name)

if __name__ == "__main__":
    # è­¦å‘Šï¼šè¯·åœ¨ llm_factory.py ä¸­å¡«å…¥æ‚¨è‡ªå·±çš„ API å¯†é’¥ï¼
    print("*"*60)
    print("è­¦å‘Š: è¯·ç¡®ä¿å·²åœ¨ 'src/llm_factory.py' ä¸­å¡«å…¥æœ‰æ•ˆçš„APIå¯†é’¥ã€‚")
    print("*"*60)
    
    asyncio.run(main())

```
<!-- [END OF FILE: src/main.py] -->

è¿™ä¸ªå®Œæ•´çš„æ¡†æ¶æ¸…æ™°åœ°å±•ç¤ºäº†å¦‚ä½•é€šè¿‡æŠ½è±¡ã€å·¥å‚å’Œé€‚é…å™¨æ¨¡å¼æ¥æ„å»ºä¸€ä¸ªå¯æ‰©å±•ã€æ˜“äºç»´æŠ¤çš„å¤šæœåŠ¡å•†LLMè°ƒç”¨å±‚ã€‚æ‚¨å¯ä»¥è½»æ¾åœ°åœ¨æ­¤åŸºç¡€ä¸Šæ·»åŠ æ–°çš„æœåŠ¡æˆ–é€‚é…å™¨ï¼Œè€Œæ— éœ€æ”¹åŠ¨ä»»ä½•ä¸Šå±‚ä¸šåŠ¡é€»è¾‘ä»£ç ã€‚
