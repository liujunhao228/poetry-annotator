[LLM]
# 默认模型配置别名
default_model = deepseek-chat
# 并发设置
max_workers = 1
max_model_pipelines = 1
# 重试设置
max_retries = 1
retry_delay = 1

[Database]
# 数据库配置
db_path = poetry.db

[Data]
# 数据路径配置
source_dir = data/source_json
output_dir = data/output

[Categories]
# 情感分类配置
xml_path = config/emotion_categories.xml
md_path = config/中国古典诗词情感分类体系.md

[Prompt]
# 提示词配置（全局默认）
template_path = config/prompt_template.txt
system_prompt_instruction_template = config/system_prompt_instruction.txt
system_prompt_example_template = config/system_prompt_example.txt
user_prompt_template = config/user_prompt_template.txt

[Logging]
# 控制台日志级别 (可选值: DEBUG, INFO, WARNING, ERROR)
# INFO: (推荐) 在控制台或GUI界面显示关键进度信息，如任务开始/结束、批次完成等。
# DEBUG: 用于开发调试，会在控制台显示所有日志，包括冗长的API请求/响应细节。
console_log_level = INFO

# 文件日志级别 (可选值: DEBUG, INFO, WARNING, ERROR)
# DEBUG: (推荐) 将所有详细信息（包括API细节、内部状态等）完整记录到文件中，便于问题追溯。
file_log_level = DEBUG

# 是否启用控制台日志输出 (true / false)
# 在后台安静运行或作为库被调用时，可设为 false。
enable_console_log = true
# 是否启用文件日志输出 (true / false)
# 强烈建议保持为 true，以便在出现问题时有据可查。
enable_file_log = true
# 日志文件路径
# 如果留空，程序将在项目根目录的 'logs' 文件夹下按时间戳自动创建文件。
log_file = 
# 单个日志文件的最大尺寸 (单位: MB)
# 当日志文件达到此大小时，会自动重命名为备份文件。
max_file_size = 10
# 保留的旧日志文件数量（日志滚动备份数）
# 例如，设为 5，则会保留 poetry_annotator.log.1, .2, .3, .4, .5 这5个备份。
backup_count = 999
# --- 其他设置 ---
# 是否静音第三方库（如 httpx, urllib3, requests）的日志 (true / false)
# 推荐设为 true，可以屏蔽掉大量与网络请求相关的底层日志，使您的日志文件更聚焦于应用本身。
quiet_third_party = true


# --- Model Configurations ---
# 使用 [Model.<your_alias>] 来定义不同的模型配置
# <your_alias> 是你在命令行中使用的别名，例如 --model deepseek-chat

[Model.DeepSeek-R1-0528-Qwen3-8B]
provider = siliconflow
model_name = deepseek-ai/DeepSeek-R1-0528-Qwen3-8B
api_key = 
base_url = https://api.siliconflow.cn/v1/chat/completions
temperature = 1.0
max_tokens = 8000
timeout = 300
# 模型特定的提示词模板配置（必选）
system_prompt_instruction_template = config/system_prompt_instruction.txt
system_prompt_example_template = config/system_prompt_example.txt
user_prompt_template = config/user_prompt_template.txt

[Model.Qwen3-235B-A22B-Instruct-2507]
provider = siliconflow
model_name = Qwen/Qwen3-235B-A22B-Instruct-2507
api_key = 
base_url = https://api-inference.modelscope.cn/v1/chat/completions
temperature = 1.0
max_tokens = 6000
timeout = 300
# 模型特定的提示词模板配置
system_prompt_instruction_template = config/system_prompt_instruction.txt
system_prompt_example_template = config/system_prompt_example.txt
user_prompt_template = config/user_prompt_template.txt

[Model.gemini-2.5-flash]
provider = gemini
model_name = models/gemini-2.5-flash
api_key = 
base_url = https://generativelanguage.googleapis.com
temperature = 1.0
max_tokens = 1024
timeout = 480
# 模型特定的提示词模板配置
system_prompt_instruction_template = config/system_prompt_instruction.txt
system_prompt_example_template = config/system_prompt_example.txt
user_prompt_template = config/user_prompt_template.txt

[Model.Qwen3-30B-A3B-Thinking-2507]
provider = siliconflow
model_name = Qwen/Qwen3-30B-A3B-Thinking-2507
api_key = 
base_url = https://api-inference.modelscope.cn/v1/chat/completions
temperature = 1.0
max_tokens = 65535
timeout = 480
# 模型特定的提示词模板配置（必选）
system_prompt_instruction_template = config/system_prompt_instruction.txt
system_prompt_example_template = config/system_prompt_example.txt
user_prompt_template = config/user_prompt_template.txt

