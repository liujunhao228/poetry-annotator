[LLM]
# --- 并发与容错控制 ---
# 模型内最大并发工作线程数（控制单个Annotator实例的并发请求数）
max_workers = 1
# 模型间最大并发管道数（控制distribute_tasks.py同时运行多少个模型任务）
max_model_pipelines = 1
# 单次API调用失败后的最大重试次数
max_retries = 3
# 重试时的基础等待时间乘数（秒），将用于指数退避策略
retry_delay = 5
# 熔断器：在连续多少次失败后开启熔断
breaker_fail_max = 3
# 熔断器：熔断开启后，多少秒后进入半开状态尝试恢复
breaker_reset_timeout = 60
# 是否保存完整的LLM响应内容（可选功能）
save_full_response = true

[Database]
# --- 数据库配置 ---
db_paths = TangShi=data/TangShi.db,SongCi=data/SongCi.db

[Data]
# 数据路径配置
source_dir = data/source_json
output_dir = data/output
[Categories]
# --- 情感分类体系配置 ---
# 情感分类体系的Markdown源文件路径
md_path = config/中国古典诗词主题分类体系.md
# （自动生成）情感体系的XML缓存文件路径
xml_path = config/emotion_categories.xml

[Prompt]
# 提示词配置（全局默认）
template_path = config/prompt_template.txt
system_prompt_instruction_template = config/system_prompt_instruction.txt
system_prompt_example_template = config/system_prompt_example.txt
user_prompt_template = config/user_prompt_template.txt

[Logging]
# --- 日志系统配置 ---
# 控制台显示的最低日志级别 (DEBUG, INFO, WARNING, ERROR)
console_log_level = INFO
# 文件记录的最低日志级别，建议设为DEBUG以捕获所有详细信息
file_log_level = DEBUG
# 是否启用文件日志
enable_file_log = True
# 日志文件路径，如果留空，将自动在 `logs/` 目录下创建带时间戳的文件
log_file = 
# 是否启用控制台日志
enable_console_log = True
# 日志文件轮转前的最大大小（MB）
max_file_size = 100
# 保留的旧日志文件数量
backup_count = 9999
# 是否静音第三方库（如httpx, urllib3）的冗余日志
quiet_third_party = True

[Visualizer]
# --- 数据可视化配置 ---
# 是否启用自定义下载功能（需要额外性能）
# 默认为 false，因为此功能不是常用功能且消耗较多资源
enable_custom_download = false

# --- Model Configurations ---
# 使用 [Model.<your_alias>] 来定义不同的模型配置
# <your_alias> 是你在命令行中使用的别名，例如 --model deepseek-chat

[Model.DeepSeek-R1-0528-Qwen3-8B]
provider = siliconflow
model_name = deepseek-ai/DeepSeek-R1-0528-Qwen3-8B
api_key = 
base_url = https://api.siliconflow.cn/v1/chat/completions
request_delay = 1.0
temperature = 1.0
max_tokens = 8000
timeout = 480
# 模型特定的提示词模板配置（必选）
system_prompt_instruction_template = config/system_prompt_instruction.txt
system_prompt_example_template = config/system_prompt_example.txt
user_prompt_template = config/user_prompt_template.txt

[Model.Qwen3-235B-A22B-Thinking-2507]
provider = dashscope
model_name = qwen3-235b-a22b-thinking-2507
api_key = 
base_url = https://dashscope.aliyuncs.com/compatible-mode/v1
request_delay = 8.0
temperature = 1.0
timeout = 480
stream = true
# 模型特定的提示词模板配置
system_prompt_instruction_template = config/system_prompt_instruction.txt
system_prompt_example_template = config/system_prompt_example.txt
user_prompt_template = config/user_prompt_template.txt

[Model.Qwen3-235B-A22B-Instruct-2507]
provider = siliconflow
model_name = Qwen/Qwen3-235B-A22B-Instruct-2507
api_key = 
base_url = https://api-inference.modelscope.cn/v1/chat/completions
request_delay = 1.0
temperature = 1.0
max_tokens = 9000
timeout = 300
# 模型特定的提示词模板配置
system_prompt_instruction_template = config/system_prompt_instruction.txt
system_prompt_example_template = config/system_prompt_example.txt
user_prompt_template = config/user_prompt_template.txt

[Model.qwen-max]
provider = dashscope
model_name = qwen-max
api_key = 
base_url = https://dashscope.aliyuncs.com/compatible-mode/v1
request_delay = 1.0
timeout = 300
enable_thinking = true
# 模型特定的提示词模板配置
system_prompt_instruction_template = config/system_prompt_instruction.txt
system_prompt_example_template = config/system_prompt_example.txt
user_prompt_template = config/user_prompt_template.txt

[Model.gemini-2.5-flash]
provider = gemini
model_name = models/gemini-2.5-flash
api_key = 
base_url = https://generativelanguage.googleapis.com
request_delay = 1.0
temperature = 1.0
max_tokens = 65535
timeout = 480
# 模型特定的提示词模板配置
system_prompt_instruction_template = config/system_prompt_instruction.txt
system_prompt_example_template = config/system_prompt_example.txt
user_prompt_template = config/user_prompt_template.txt

[Model.DeepSeek-R1]
provider = siliconflow
model_name = deepseek-ai/DeepSeek-R1-0528
api_key = 
base_url = https://api-inference.modelscope.cn/v1/chat/completions
request_delay = 1.0
temperature = 1.0
max_tokens = 8000
timeout = 480
# 模型特定的提示词模板配置
system_prompt_instruction_template = config/system_prompt_instruction.txt
system_prompt_example_template = config/system_prompt_example.txt
user_prompt_template = config/user_prompt_template.txt

[Model.Qwen3-30B-A3B-Thinking-2507]
provider = siliconflow
model_name = Qwen/Qwen3-30B-A3B-Thinking-2507
api_key = 
base_url = https://api-inference.modelscope.cn/v1/chat/completions
request_delay = 1.0
temperature = 1.0
max_tokens = 9000
timeout = 300
# 模型特定的提示词模板配置
system_prompt_instruction_template = config/system_prompt_instruction.txt
system_prompt_example_template = config/system_prompt_example.txt
user_prompt_template = config/user_prompt_template.txt